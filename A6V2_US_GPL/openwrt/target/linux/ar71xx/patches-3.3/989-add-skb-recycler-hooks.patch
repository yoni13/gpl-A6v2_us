--- a/net/Kconfig
+++ b/net/Kconfig
@@ -266,6 +266,19 @@ config BPF_JIT
 	  packet sniffing (libpcap/tcpdump). Note : Admin should enable
 	  this feature changing /proc/sys/net/core/bpf_jit_enable
 
+config SKB_RECYCLER
+	bool "Generic skb recycling"
+	default y
+	---help---
+	 Enables RX-to-RX skb recycling scheme for bridging and
+	 routing workloads, reducing skbuff freeing/reallocation
+	 overhead.
+
+config SKB_RECYCLER_MULTI_CPU
+	bool "Cross-CPU recycling for CPU-locked workloads"
+	depends on SMP && SKB_RECYCLER
+	default n
+
 menu "Network testing"
 
 config NET_PKTGEN
--- a/net/core/Makefile
+++ b/net/core/Makefile
@@ -21,3 +21,4 @@ obj-$(CONFIG_TRACEPOINTS) += net-traces.
 obj-$(CONFIG_NET_DROP_MONITOR) += drop_monitor.o
 obj-$(CONFIG_NETWORK_PHY_TIMESTAMPING) += timestamping.o
 obj-$(CONFIG_NETPRIO_CGROUP) += netprio_cgroup.o
+obj-$(CONFIG_SKB_RECYCLER) += skbuff_recycle.o
--- a/net/core/skbuff.c
+++ b/net/core/skbuff.c
@@ -71,6 +71,7 @@
 #include <trace/events/skb.h>
 
 #include "kmap_skb.h"
+#include "skbuff_recycle.h"
 
 static struct kmem_cache *skbuff_head_cache __read_mostly;
 static struct kmem_cache *skbuff_fclone_cache __read_mostly;
@@ -310,9 +311,20 @@ EXPORT_SYMBOL(build_skb);
 struct sk_buff *__netdev_alloc_skb(struct net_device *dev,
 		unsigned int length, gfp_t gfp_mask)
 {
+	unsigned int len;
 	struct sk_buff *skb;
 
-	skb = __alloc_skb(length + NET_SKB_PAD, gfp_mask, 0, NUMA_NO_NODE);
+	skb = skb_recycler_alloc(dev, length);
+
+	if (likely(skb)) {
+		return skb;
+	}
+
+	len = SKB_RECYCLE_SIZE;
+	if (unlikely(length > SKB_RECYCLE_SIZE))
+		len = length;
+
+	skb = __alloc_skb(len + NET_SKB_PAD, gfp_mask, 0, NUMA_NO_NODE);
 	if (likely(skb)) {
 		skb_reserve(skb, NET_SKB_PAD);
 		skb->dev = dev;
@@ -361,11 +373,22 @@ EXPORT_SYMBOL(skb_add_rx_frag);
  */
 struct sk_buff *dev_alloc_skb(unsigned int length)
 {
+	unsigned int len;
+
+	struct sk_buff *skb = skb_recycler_alloc(NULL, length);
+
+	if (likely(skb)) {
+		return skb;
+	}
+
+	len = SKB_RECYCLE_SIZE;
+	if (unlikely(length > SKB_RECYCLE_SIZE))
+		len = length;
 	/*
 	 * There is more code here than it seems:
 	 * __dev_alloc_skb is an inline
 	 */
-	return __dev_alloc_skb(length, GFP_ATOMIC);
+	return __dev_alloc_skb(len, GFP_ATOMIC);
 }
 EXPORT_SYMBOL(dev_alloc_skb);
 
@@ -395,7 +418,7 @@ static void skb_clone_fraglist(struct sk
 		skb_get(list);
 }
 
-static void skb_release_data(struct sk_buff *skb)
+void skb_release_data(struct sk_buff *skb)
 {
 	if (!skb->cloned ||
 	    !atomic_sub_return(skb->nohdr ? (1 << SKB_DATAREF_SHIFT) + 1 : 1,
@@ -428,7 +451,7 @@ static void skb_release_data(struct sk_b
 /*
  *	Free an skbuff by memory without cleaning the state.
  */
-static inline void kfree_skbmem(struct sk_buff *skb)
+inline void kfree_skbmem(struct sk_buff *skb)
 {
 	struct sk_buff *other;
 	atomic_t *fclone_ref;
@@ -542,12 +565,40 @@ void consume_skb(struct sk_buff *skb)
 {
 	if (unlikely(!skb))
 		return;
+
+	prefetch(&skb->destructor);
+
 	if (likely(atomic_read(&skb->users) == 1))
 		smp_rmb();
 	else if (likely(!atomic_dec_and_test(&skb->users)))
 		return;
+
+	/*
+	 * If possible we'd like to recycle any skb rather than just free it,
+	 * but in order to do that we need to release any head state too.
+	 * We don't want to do this later because we'll be in a pre-emption
+	 * disabled state.
+	 */
+	skb_release_head_state(skb);
+
+	/*
+	 * Can we recycle this skb?  If we can then it will be much faster
+	 * for us to recycle this one later than to allocate a new one
+	 * from scratch.
+	 */
+	if (likely(skb_recycler_consume(skb))) {
+		return;
+	}
+
 	trace_consume_skb(skb);
-	__kfree_skb(skb);
+
+	/*
+	 * We're not recycling so now we need to do the rest of what we would
+	 * have done in __kfree_skb (above and beyond the skb_release_head_state
+	 * that we already did.
+	 */
+	skb_release_data(skb);
+	kfree_skbmem(skb);
 }
 EXPORT_SYMBOL(consume_skb);
 
@@ -2975,6 +3026,7 @@ void __init skb_init(void)
 						0,
 						SLAB_HWCACHE_ALIGN|SLAB_PANIC,
 						NULL);
+	skb_recycler_init();
 }
 
 /**
--- a/include/linux/skbuff.h
+++ b/include/linux/skbuff.h
@@ -548,6 +548,8 @@ static inline struct rtable *skb_rtable(
 }
 
 extern void kfree_skb(struct sk_buff *skb);
+extern void kfree_skbmem(struct sk_buff *skb);
+extern void skb_release_data(struct sk_buff *skb);
 extern void consume_skb(struct sk_buff *skb);
 extern void	       __kfree_skb(struct sk_buff *skb);
 extern struct sk_buff *__alloc_skb(unsigned int size,
